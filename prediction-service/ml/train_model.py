"""
æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒè„šæœ¬
ä½¿ç”¨ RandomForestClassifier è®­ç»ƒè¶³çƒæ¯”èµ›ç»“æœé¢„æµ‹æ¨¡å‹
"""

import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import joblib


def load_data(csv_path: str) -> tuple:
    """åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®"""
    print(f"ğŸ“‚ åŠ è½½æ•°æ®: {csv_path}")
    
    df = pd.read_csv(csv_path)
    print(f"   æ€»æ ·æœ¬æ•°: {len(df)}")
    
    # ç‰¹å¾åˆ—
    feature_columns = [
        'home_goals',
        'away_goals', 
        'minute',
        'home_shots_on_target',
        'away_shots_on_target',
        'red_cards'
    ]
    
    X = df[feature_columns]
    y = df['result']
    
    return X, y, feature_columns


def train_model(X: pd.DataFrame, y: pd.Series) -> RandomForestClassifier:
    """è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹"""
    print("\nğŸŒ² å¼€å§‹è®­ç»ƒ RandomForest æ¨¡å‹...")
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"   è®­ç»ƒé›†å¤§å°: {len(X_train)}")
    print(f"   æµ‹è¯•é›†å¤§å°: {len(X_test)}")
    
    # åˆ›å»ºéšæœºæ£®æ—åˆ†ç±»å™¨
    model = RandomForestClassifier(
        n_estimators=100,      # 100 æ£µæ ‘
        max_depth=10,          # æœ€å¤§æ·±åº¦ 10
        min_samples_split=5,   # æœ€å°åˆ†è£‚æ ·æœ¬æ•°
        min_samples_leaf=2,    # å¶èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°
        random_state=42,
        n_jobs=-1,             # ä½¿ç”¨æ‰€æœ‰ CPU æ ¸å¿ƒ
        class_weight='balanced' # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
    )
    
    # è®­ç»ƒæ¨¡å‹
    print("   è®­ç»ƒä¸­...")
    model.fit(X_train, y_train)
    
    # è¯„ä¼°æ¨¡å‹
    print("\nğŸ“Š æ¨¡å‹è¯„ä¼°:")
    
    # è®­ç»ƒé›†å‡†ç¡®ç‡
    train_pred = model.predict(X_train)
    train_acc = accuracy_score(y_train, train_pred)
    print(f"   è®­ç»ƒé›†å‡†ç¡®ç‡: {train_acc * 100:.2f}%")
    
    # æµ‹è¯•é›†å‡†ç¡®ç‡
    test_pred = model.predict(X_test)
    test_acc = accuracy_score(y_test, test_pred)
    print(f"   æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc * 100:.2f}%")
    
    # äº¤å‰éªŒè¯
    cv_scores = cross_val_score(model, X, y, cv=5)
    print(f"   5æŠ˜äº¤å‰éªŒè¯: {cv_scores.mean() * 100:.2f}% (+/- {cv_scores.std() * 2 * 100:.2f}%)")
    
    # åˆ†ç±»æŠ¥å‘Š
    print("\nğŸ“‹ åˆ†ç±»æŠ¥å‘Š:")
    target_names = ['ä¸»èƒœ (0)', 'å¹³å±€ (1)', 'å®¢èƒœ (2)']
    print(classification_report(y_test, test_pred, target_names=target_names))
    
    # æ··æ·†çŸ©é˜µ
    print("ğŸ”¢ æ··æ·†çŸ©é˜µ:")
    cm = confusion_matrix(y_test, test_pred)
    print(f"   é¢„æµ‹ â†’    ä¸»èƒœ  å¹³å±€  å®¢èƒœ")
    print(f"   å®é™… â†“")
    print(f"   ä¸»èƒœ      {cm[0][0]:4d}  {cm[0][1]:4d}  {cm[0][2]:4d}")
    print(f"   å¹³å±€      {cm[1][0]:4d}  {cm[1][1]:4d}  {cm[1][2]:4d}")
    print(f"   å®¢èƒœ      {cm[2][0]:4d}  {cm[2][1]:4d}  {cm[2][2]:4d}")
    
    # ç‰¹å¾é‡è¦æ€§
    print("\nğŸ¯ ç‰¹å¾é‡è¦æ€§:")
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    for _, row in feature_importance.iterrows():
        bar = 'â–ˆ' * int(row['importance'] * 50)
        print(f"   {row['feature']:25s} {row['importance']:.4f} {bar}")
    
    return model


def save_model(model: RandomForestClassifier, output_path: str, feature_columns: list):
    """ä¿å­˜æ¨¡å‹å’Œå…ƒæ•°æ®"""
    print(f"\nğŸ’¾ ä¿å­˜æ¨¡å‹åˆ°: {output_path}")
    
    # ä¿å­˜æ¨¡å‹å’Œç‰¹å¾åˆ—å
    model_data = {
        'model': model,
        'feature_columns': feature_columns,
        'version': 'v1.0',
        'classes': ['home_win', 'draw', 'away_win']
    }
    
    joblib.dump(model_data, output_path)
    
    file_size = Path(output_path).stat().st_size / 1024
    print(f"   æ–‡ä»¶å¤§å°: {file_size:.1f} KB")


def test_prediction(model: RandomForestClassifier):
    """æµ‹è¯•æ¨¡å‹é¢„æµ‹"""
    print("\nğŸ§ª æµ‹è¯•é¢„æµ‹:")
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        # [home_goals, away_goals, minute, home_shots, away_shots, red_cards]
        {"name": "ä¸»é˜Ÿé¢†å…ˆ 2-0 (60åˆ†é’Ÿ)", "features": [2, 0, 60, 8, 3, 0]},
        {"name": "å®¢é˜Ÿé¢†å…ˆ 0-2 (70åˆ†é’Ÿ)", "features": [0, 2, 70, 2, 9, 0]},
        {"name": "å¹³å±€ 1-1 (45åˆ†é’Ÿ)", "features": [1, 1, 45, 5, 5, 0]},
        {"name": "å¹³å±€ 0-0 (80åˆ†é’Ÿ)", "features": [0, 0, 80, 3, 4, 0]},
        {"name": "ä¸»é˜Ÿé¢†å…ˆä½†å°‘äºº (çº¢ç‰Œ)", "features": [1, 0, 50, 4, 6, 1]},
    ]
    
    for case in test_cases:
        X = np.array([case['features']])
        proba = model.predict_proba(X)[0]
        pred = model.predict(X)[0]
        result_map = {0: 'ä¸»èƒœ', 1: 'å¹³å±€', 2: 'å®¢èƒœ'}
        
        print(f"\n   åœºæ™¯: {case['name']}")
        print(f"   é¢„æµ‹: {result_map[pred]}")
        print(f"   æ¦‚ç‡: ä¸»èƒœ {proba[0]*100:.1f}% | å¹³å±€ {proba[1]*100:.1f}% | å®¢èƒœ {proba[2]*100:.1f}%")


def main():
    print("=" * 60)
    print("ğŸ¤– è¶³çƒæ¯”èµ›ç»“æœé¢„æµ‹æ¨¡å‹è®­ç»ƒ")
    print("=" * 60)
    
    # è·¯å¾„è®¾ç½®
    ml_dir = Path(__file__).parent
    csv_path = ml_dir / "historical_matches.csv"
    model_path = ml_dir / "model_v1.pkl"
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not csv_path.exists():
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {csv_path}")
        print("   è¯·å…ˆè¿è¡Œ: python generate_dummy_data.py")
        return
    
    # åŠ è½½æ•°æ®
    X, y, feature_columns = load_data(csv_path)
    
    # è®­ç»ƒæ¨¡å‹
    model = train_model(X, y)
    
    # ä¿å­˜æ¨¡å‹
    save_model(model, model_path, feature_columns)
    
    # æµ‹è¯•é¢„æµ‹
    test_prediction(model)
    
    print("\n" + "=" * 60)
    print("âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ° model_v1.pkl")
    print("=" * 60)


if __name__ == "__main__":
    main()
